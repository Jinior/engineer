{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "Joaquin Vanschoren, Eindhoven University of Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras 2.1.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:105% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global imports and settings\n",
    "from preamble import *\n",
    "import keras\n",
    "print(\"Using Keras\",keras.__version__)\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 125 # Use 300 for PDF, 100 for slides\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:105% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>''') # For slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mathematical Foundations\n",
    "* A first example\n",
    "* Tensors and tensor operations\n",
    "* Backpropagation and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A first example: classifying digits\n",
    "- This example is meant to introduce the main concepts. We'll cover them in more detail later.\n",
    "- MNIST dataset contains 28x28 pixel images of handwritten digits (0-9)\n",
    "- The goal is to classify each image as one of the possible digits\n",
    "- We __reshape__ the data to a 70000x28x28 __tensor__ (n-dimensional matrix)  \n",
    "    `X = X.reshape(70000, 28, 28)`\n",
    "- Traditional holdout uses the last 10,000 images for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1250x625 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download MINST data. Takes a while the first time.\n",
    "mnist = oml.datasets.get_dataset(554)\n",
    "X, y = mnist.get_data(target=mnist.default_target_attribute);\n",
    "X = X.reshape(70000, 28, 28)\n",
    "\n",
    "# Take some random examples\n",
    "from random import randint\n",
    "fig, axes = plt.subplots(1, 5,  figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    n = randint(0,70000)\n",
    "    axes[i].imshow(X[n], cmap=plt.cm.gray_r)\n",
    "    axes[i].set_xlabel(\"Label: {}\".format(y[n]))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (60000, 28, 28)\n",
      "Test set:  (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=60000)\n",
    "print(\"Training set: \",X_train.shape)\n",
    "print(\"Test set: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note: this is also one of the datasets that comes included with Keras:\n",
    "``` python\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Neural networks\n",
    "* The core building block of a neural network is the _layer_\n",
    "* You can think of it as a _filter_ for the data\n",
    "    - Data goes in, and comes out in a more useful form\n",
    "* Layers extract new _representations_ of the data\n",
    "* _Deep learning_ models contain many such layers\n",
    "    - They progressively _distill_ (refine) the data\n",
    "<img src=\"../images/00_layers.png\" alt=\"ml\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The pixel values are fed to individual _nodes_ of the _input layer_ (yellow)\n",
    "* The data then passes through one or more _hidden layers_ (blue)\n",
    "    - One type of layer is the _dense_ or _fully connected_ layer\n",
    "    - Every node is connected to all nodes in the previous and subsequent layers\n",
    "* The _output layer_ has a node for every possible outcome (digits 0-9) (red)\n",
    "    - I.e. The first node returns the probability that the input image represents a '0'\n",
    "<img src=\"../images/08_nn_manylayers.png\" alt=\"ml\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The perceptron\n",
    "* In its simplest form, each node outputs a weighted sum of the inputs: $y = \\sum_{i} x_{i}w_i + b$\n",
    "* It needs to learn the optimal set of weights to produce the right output\n",
    "    * _Bias_ $b$: modelled as the weight of an extra input that's always '1'\n",
    "* This is the same as a linear model, can only learn linear decision boundaries.\n",
    "    * Even a deep neural net of perceptrons can only learn a linear model\n",
    "<img src=\"../images/08_log_reg_nn.png\" alt=\"ml\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Activation functions\n",
    "* To learn a non-linear model, each hidden node has to output a non-linear _activation function_ $f$ on the weighted sum of the inputs: $h(x)=f(W_1 x+b_1)$\n",
    "* Likewise, the output nodes use an activation function $g$ on the weighted outputs of the previous layer: $o(x)=g(W_2 h(x)+b_2)$\n",
    "<img src=\"../images/08_nn_basic_arch.png\" alt=\"ml\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Activation functions\n",
    "* For hidden nodes, popular choices are the _rectified linear unit_ (ReLU) and _tanh_\n",
    "    - There are many others. We'll come back to this soon!\n",
    "    - ReLU is very cheap to compute, speeds up training\n",
    "* For classification, we use _softmax_ (or sigmoid)\n",
    "    - Transforms the input into a probability for (each specific outcome)\n",
    "    - This is exactly what we used for logistic regression!\n",
    "<img src=\"../images/08_activation.png\" alt=\"ml\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now build a simple neural network for MNIST:\n",
    "* One dense hidden ReLU layer with 512 nodes\n",
    "    - Input from a 28x28 matrix\n",
    "* Output softmax layer with 10 nodes\n",
    "\n",
    "``` python\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "'Visualize' the model using `summary()` \n",
    "- Also shows the number of model parameters (weights) that need to be learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Compilation\n",
    "We still need to specify how we want the network to be trained:\n",
    "* __Loss function__: The objective function used to measure how well the model is doing, and steer itself in the right direction\n",
    "    - e.g. Cross Entropy (_negative log likelihood_ or _log loss_) for classification\n",
    "* __Optimizer__: How to optimize the model weights in every iteration.\n",
    "    - usually a [variant of stochastic gradient descent](http://ruder.io/optimizing-gradient-descent/index.html#momentum)\n",
    "* __Metrics__ to monitor performance during training and testing.\n",
    "    - e.g. accuracy\n",
    "    \n",
    "``` python\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Preprocessing \n",
    "* Neural networks are sensitive to scaling, so always scale the inputs\n",
    "* The network expects the data in shape (n, 28 * 28), so we also need to reshape\n",
    "* We also need to categorically encode the labels\n",
    "    - e.g. '4' becomes [0,0,0,0,1,0,0,0,0,0]\n",
    "\n",
    "``` python\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((60000, 28 * 28))\n",
    "X_test = X_test.reshape((10000, 28 * 28))\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training\n",
    "Training (fitting) is done by __stochastic gradient descent__ (SGD).\n",
    "* Optimizes the model parameters (weights)\n",
    "* We'll come back to this soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0281 - acc: 0.9916\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0208 - acc: 0.9939\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0156 - acc: 0.9955\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0121 - acc: 0.9965\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0098 - acc: 0.9972\n"
     ]
    }
   ],
   "source": [
    "network.fit(X_train, y_train, epochs=5, batch_size=128);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Prediction\n",
    "We can now call `predict` or `predict_proba` to generate predictions\n",
    "\n",
    "``` python\n",
    "np.set_printoptions(precision=7)\n",
    "print(\"Prediction: \",network.predict(X_test)[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0.        0.0000002 0.        0.000001  0.9863701 0.000019  0.\n",
      " 0.0000008 0.0005651 0.0130437]\n",
      "Label:  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=7)\n",
    "print(\"Prediction: \",network.predict(X_test)[0])\n",
    "print(\"Label: \",y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluation\n",
    "Evaluate the trained model on the entire test set\n",
    "\n",
    "``` python\n",
    "test_loss, test_acc = network.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 57us/step\n",
      "Test accuracy: 0.9793\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Overfitting\n",
    "* Our test set accuracy is quite a bit lower than our training set accuracy\n",
    "* We've already seen many choices (moving pieces) that can still be optimized:\n",
    "    - Number of layers\n",
    "    - Number of nodes per layer\n",
    "    - Activation functions\n",
    "    - Loss function (and hyperparameters)\n",
    "    - SGD optimizer (and hyperparameters)\n",
    "    - Batch size\n",
    "    - Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensors and tensor operations\n",
    "Representing data and learning better representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tensors\n",
    "* A _tensor_ is simply an n-dimensional array (with n axes)\n",
    "    * 2D tensor: matrix (samples, features)\n",
    "    * 3D tensor: grayscale images (samples, height, width)\n",
    "        - or time series (samples, timesteps, features)\n",
    "    * 4D tensor: color images (samples, height, width, channels)\n",
    "    * 5D tensor: video (amples, frames, height, width, channels)  \n",
    "    \n",
    "<img src=\"../images/08_timeseries.png\" alt=\"ml\" style=\"float: left; width: 30%;\"/>\n",
    "<img src=\"../images/08_images.png\" alt=\"ml\" style=\"float: left; width: 30%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tensor operations\n",
    "The operations that neural network layers perform on the data can be reduced to a handful of tensor operations.\n",
    "\n",
    "``` python\n",
    "keras.layers.Dense(512, activation='relu') \n",
    "```\n",
    "\n",
    "can be interpreted as a function\n",
    "\n",
    "``` python\n",
    "y = relu(dot(W, x) + b)\n",
    "```\n",
    "\n",
    "* takes a 2D tensor $x$ and returns a new 2D tensor $y$\n",
    "* uses a 2D weight tensor $W$ and a bias vector $b$\n",
    "* performs a dot product, addition, and $relu(x) = max(x,0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Element-wise operations\n",
    "\n",
    "ReLU and addition are element-wise operations. Since numpy arrays support element-wise operations natively, these are simply:\n",
    "\n",
    "``` python\n",
    "def relu(x):\n",
    "  return np.maximum(x, 0.)\n",
    "\n",
    "def add(x, y):\n",
    "  return x + y\n",
    "```\n",
    "\n",
    "Note: if y has a lower dimension than x, it will be _broadcasted_: axes are added to match the dimensionality, and y is repeated along the new axes \n",
    "\n",
    "``` python\n",
    ">>> np.array([[1,2],[3,4]]) + np.array([10,20])\n",
    "array([[11, 22],\n",
    "       [13, 24]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tensor dot\n",
    "The dot product $x . y$ of two tensors can also be done easily with numpy:\n",
    "``` python \n",
    "z = np.dot(x, y)\n",
    "```\n",
    "where \n",
    "``` python \n",
    "z[i,j] = x[i,:] * y[:,j]\n",
    "```\n",
    "\n",
    "<img src=\"../images/08_dot.png\" alt=\"ml\" style=\"float: left; width: 30%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Geometric interpretation\n",
    "* Dot products and additions change how data points relate to each other\n",
    "* We aim to find a transformation of the data points so that it becomes easy to:\n",
    "    - separate the classes (classification)\n",
    "    - learn a simple function (regression)\n",
    "\n",
    "<img src=\"../images/08_addition.png\" alt=\"ml\" style=\"float: left; width: 25%;\"/>\n",
    "<img src=\"../images/08_dotgeom.png\" alt=\"ml\" style=\"float: left; width: 30%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient-based optimization\n",
    "* We saw that a layer performs an operation like:\n",
    "``` python\n",
    "y = relu(dot(W, x) + b)\n",
    "```\n",
    "\n",
    "* How to find good values for $W$ and $b$ so that the data is transformed to a useful representation?\n",
    "* Start with a random initialization, then loop:\n",
    "    1. Draw a batch of training data $x$\n",
    "    2. _Forward pass_: run the network on $x$ to yield $y_{pred}$ (tensor operations)\n",
    "    3. Compute the loss (mismatch between  $y_{pred}$ and $y$)\n",
    "    4. Update $W$, $b$ in a way that slightly reduces the loss (OK, but how?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Update rule\n",
    "Naive approach (expensive):\n",
    "* Choose one weight $w_{i,j}$ to optimize, freeze the others\n",
    "* Run the network (twice) with $w_{i,j} - \\epsilon$ and $w_{i,j} + \\epsilon$\n",
    "* Compute the losses given current batch x\n",
    "* Keep the one that reduces the loss most, then repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../images/08_gradient.png\" alt=\"ml\" style=\"float: right; width: 30%;\"/>\n",
    "\n",
    "Better:\n",
    "* Choose a loss function f that is _differentiable_\n",
    "    * Also all underlying tensor operations need to be differentiable\n",
    "* Then we can compute the derivative $\\frac{\\partial f(x,w_{i,j})}{\\partial w_{i,j}} = a$\n",
    "* So that $f(x,w_{i,j} + \\epsilon) = y + a * \\epsilon$\n",
    "* We can now estimate better weights without recomputing $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gradients\n",
    "* A _gradient_ is the generalization of a derivate to n-dimensional inputs\n",
    "    * Approximates the _curvature_ of the loss function $f(x,W)$ around a given point $W$\n",
    "* Update: if $f$ is differentiable, then $W_1$ = $W_0$ - $\\frac{\\partial f(W_0)}{\\partial W} * step$ \n",
    "    * step is a small scaling factor\n",
    "    * Go against the curvature to a lower place on the curve\n",
    "* Now repeat with a new batch of data $x$\n",
    "\n",
    "<img src=\"../images/08_2dgradient.png\" alt=\"ml\" style=\"float: left; width: 40%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stochastic gradient descent (SGD)\n",
    "Mini-batch SGD:\n",
    "1. Draw a batch of *batch_size* training data $x$ and $y$\n",
    "2. _Forward pass_: run the network on $x$ to yield $y_{pred}$ (tensor operations)\n",
    "3. Compute the loss L (mismatch between  $y_{pred}$ and $y$)\n",
    "4. _Backward pass_: Compute the gradient of the loss with regard to $W$\n",
    "5. Update W: $W_{i+1} = W_i - \\frac{\\partial L(x, W_i)}{\\partial W} * step$\n",
    "\n",
    "Repeat until n passes (epochs) are made through through the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SGD Variants:\n",
    "* Batch Gradient Descent: compute gradient on entire training set\n",
    "    - More accurate gradients, but more expensive\n",
    "* True Stochastic Gradient Descent: repeat for each individual data points (noisy)\n",
    "* Minibatch SGD strikes a balance between the two (given the right batch size)\n",
    "    \n",
    "<img src=\"../images/08_sgd1d.png\" alt=\"ml\" style=\"float: left; width: 25%;\"/>\n",
    "<img src=\"../images/08_SGD.png\" alt=\"ml\" style=\"float: left; width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### SGD: many more variants\n",
    "* With SGD, it is quite easy to get stuck in a local minimum\n",
    "* Learning rate decay: start with a big step size and then decrease\n",
    "* Momentum: do a larger update if previous update has large loss improvement\n",
    "    - Like a ball that gains speed if it goes down steeply\n",
    "* Adaptive step size for each W_i: adam, Adagrad,...\n",
    "    - See http://ruder.io/optimizing-gradient-descent/index.html\n",
    "* Some intuitions say that in high-dimensional spaces, most local minima are near the global minimum\n",
    "\n",
    "<img src=\"../images/08_sgdvars.png\" alt=\"ml\" style=\"float: left; width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Backpropagation\n",
    "* In practice, a neural network function consist of many tensor operations chained together\n",
    "    - e.g. $f(W1, W2, W3) = a(W1, b(W2, c(W3)))$\n",
    "* As long as each tensor operation is differentiable, we can still compute the gradient thanks to the chain rule:\n",
    "    $$f(g(x)) = f'(g(x)) * g'(x)$$\n",
    "* We can let the gradient _backpropagate_ through the layers\n",
    "* So, if we have a hidden node $h(x)=f(W_1 x+b_1)$, $net(x) = W_1 x+b_1$, and output node $o(x)=g(W_2 h(x)+b_2)$\n",
    "\n",
    "<img src=\"../images/08_chainrule2.png\" alt=\"ml\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding $\\frac{\\partial h(x)}{\\partial net(x)}$\n",
    "- Imagine a demon that adds a little change $\\Delta u_k$ to the input of neuron $k$\n",
    "    - The neuron now outputs $\\sigma(u_k + \\Delta u_k)$ instead of $\\sigma(u_k)$\n",
    "    - Propagates through network, ultimately causing an error $\\frac{\\delta E}{\\delta u_k} \\Delta u_k$\n",
    "- A good demon helps you improve the error by trying to find a $\\Delta u_k$ that reduces the error\n",
    "    - If $\\frac{\\delta E}{\\delta u_k}$ is large, choose $\\Delta u_k$ to reduce it\n",
    "\n",
    "![Tampering with the weights creates an error downstream.](./images/07_demon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation in action\n",
    "To get an intuitive understanding of how backpropagation works, here is a nice animation of the entire process:\n",
    "    https://youtu.be/Ilg3gGewQ5U?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Symbolic and automatic differentiation\n",
    "Symbolic differentiation: given a chain of operations with a known derivative, we can compute a _gradient function_ for the chain\n",
    "* Decomposes functions into simpler functions via the chain rule\n",
    "* We can call the gradient function to get the gradient value for every model parameter\n",
    "\n",
    "Automatic differentiation: evaluate the derivate of a function numerically for faster calculation \n",
    "\n",
    "\n",
    "Modern tools such as TensorFlow do this for you so you don't have to implement backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.10/tutorials/\n",
    "\n",
    "http://playground.tensorflow.org\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.10/get_started/basic_usage\n",
    "\n",
    "https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/\n",
    "    \n",
    "https://keras.io/\n",
    "\n",
    "Tensorboard web interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
